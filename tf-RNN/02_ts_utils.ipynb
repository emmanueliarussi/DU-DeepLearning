{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "## Imports and Setup - Data prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks  import EarlyStopping\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ts_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ts_utils.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks  import EarlyStopping\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (14, 4)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    Load the data you prepared - you must have run 01_ts_dataprep \n",
    "'''\n",
    "def load_file( file = '../data/jena_climate_2009_2016.csv.zip'):\n",
    "    df = pd.read_csv(file+\".csv\")\n",
    "    df['Date Time'] = pd.to_datetime( df['Date Time'], format='%Y-%m-%d %H:%M:%S' )\n",
    "    df_scaled_trn   = pd.read_csv(file+\".trn.csv\")\n",
    "    df_scaled_tst   = pd.read_csv(file+\".tst.csv\")\n",
    "    scaler          = pickle.load(open(f'{file}.scaler.pkl', 'rb'))\n",
    "\n",
    "    return df, df_scaled_trn, df_scaled_tst, scaler\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    dataset:        must be tf.data.Dataset.from_tensor_slices\n",
    "    label_slice:    labels (indices or slice(start,end, skip) )\n",
    "    window_len:     Length of the window\n",
    "    output_len:     Length of the labels (# of steps to predict)\n",
    "\n",
    "    for_aencoder:   Note: for auto encoder, output is same as input\n",
    "\n",
    "Usage:\n",
    "    df = pd.read_csv(file) or [[0,1,2,3], [0,1,2,3], [0,1,2,3], [0,1,2,3], [0,1,2,3]]\n",
    "    ds = timeseries_dataset_from_dataset(df, 2, 2, slice(0, 2))\n",
    "    #print_dataset(ds)\n",
    "\n",
    "'''\n",
    "def window(dataset, window_len, output_length, label_slice=slice(0,1), batch_size=1, skip = 0):\n",
    "    ds = dataset.window(window_len + skip + output_length, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x).batch(window_len + skip+ output_length)\n",
    "     \n",
    "    def split_feature_label(x):\n",
    "        return x[:window_len], x[window_len+skip:,label_slice]\n",
    "     \n",
    "    ds = ds.map(split_feature_label)\n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "def windowae(dataset, window_len, batch_size=1):\n",
    "    ds = dataset.window(window_len, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x).batch(window_len)\n",
    "     \n",
    "    def split_feature_label(x):\n",
    "        return x, x\n",
    "        #return x[:window_len], x[:window_len]\n",
    "     \n",
    "    ds = ds.map(split_feature_label)\n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Compute the Average of the training output and we will use this as default predictions\n",
    "# Also for computing R-squared value\n",
    "def compute_avg(window):\n",
    "    count, total = 0, None;\n",
    "    for w in window:\n",
    "        if (not count):\n",
    "            total = w[1]\n",
    "        else:\n",
    "            total += w[1]\n",
    "        count += 1\n",
    "\n",
    "    avg_output = total/count\n",
    "    return avg_output\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    predict the model,\n",
    "    y:      is the original array of expected \n",
    "    yhat:   is the predicted values\n",
    "'''\n",
    "def model_predict(model, window, y=None, yhat= None, howmany=1024*1024):\n",
    "    for w in window.take(howmany):\n",
    "        xc = w[0]\n",
    "        yc = w[1]\n",
    "        yp = model.predict(xc, verbose=0)\n",
    "\n",
    "        yc = yc[:,-1,:]\n",
    "        yp = yp[:,-1,:]\n",
    "\n",
    "        if ( y is None):\n",
    "            y = yc\n",
    "            yhat = yp\n",
    "            continue;\n",
    "        \n",
    "        y = np.concatenate([y,yc])\n",
    "        yhat = np.concatenate([yhat,yp])\n",
    "\n",
    "    return y, yhat\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Define inv_transform functions - Note: yh: [batch, time, features length]\n",
    "def inverse_transform(yh, scaler, label_slice, df=None):\n",
    "    yy=np.empty([yh.shape[0], scaler.n_features_in_])\n",
    "    yy[:] = np.nan\n",
    "\n",
    "    yy[:, label_slice] = yh\n",
    "    ys = scaler.inverse_transform(yy)\n",
    "\n",
    "    if (df is not None):\n",
    "        ys = pd.DataFrame(ys[:, label_slice], columns=df.columns[label_slice])\n",
    "\n",
    "    return ys    \n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def compile_fit(model, window_trn=None, window_tst= None, opt=None, patience=3, epochs=1, callbacks=[], **kwargs):\n",
    "    earlyStopCB = EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights = True)\n",
    "\n",
    "    callbacks.append(earlyStopCB)\n",
    "\n",
    "    # You may use these callbacks to save the model if you wanted to\n",
    "\n",
    "    #saveModelCB = ModelCheckpoint(filepath=model.name + \".model.tf\", save_best_only=True, verbose=0)\n",
    "    #tensorBrdCB = TensorBoard(log_dir= f'./logs/{model.name}', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    opt  = opt or tf.keras.optimizers.Adam()\n",
    "    mets = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "    ##=> Other options you can try\n",
    "    #learning_rate = 1e-6\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    #opt = tf.keras.optimizers.SGD()\n",
    "    #loss=tf.keras.losses.Huber()\n",
    "\n",
    "    model.compile(loss= loss, optimizer= opt, metrics=mets)\n",
    "\n",
    "    history = []\n",
    "    if (window_trn is not None):\n",
    "        history = model.fit(window_trn, epochs=epochs, validation_data=window_tst, \n",
    "                                workers=4, use_multiprocessing=True, callbacks=callbacks, **kwargs)\n",
    "\n",
    "    return history\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    This commonLayer, a layer that is common to all models given output_len, out_features length.\n",
    "\n",
    "    suppose if you are building a prediction, forecasting model say\n",
    "    ouput_len      = 4\n",
    "    ouput_feat_len = 2\n",
    "\n",
    "    This means your output will have a final linear layer of length: \n",
    "            op_len = ouput_len * ouput_feat_len;\n",
    "    And your will have linear activation (sometimes relu makes sense or custom activation)\n",
    "\n",
    "    And you will have to reshape to compare to actual output. Instead of creating this last year,\n",
    "    you may choose to call this function for convenience.\n",
    "\n",
    "'''\n",
    "def getCommonLayer(ouput_len, ouput_feat_len, previousLayer=None, activation=\"linear\"):\n",
    "    op_len = ouput_len * ouput_feat_len;\n",
    "    commonLayer = [\n",
    "        # Shape => [batch, 1, out_len * #features]\n",
    "        tf.keras.layers.Dense( op_len, activation=activation, kernel_initializer=tf.initializers.zeros()),\n",
    "        \n",
    "        # Shape => [batch, out_steps, features]\n",
    "        tf.keras.layers.Reshape([ouput_len, ouput_feat_len])\n",
    "    ]\n",
    "    if (previousLayer is not None):\n",
    "        preds = commonLayer[0](previousLayer)\n",
    "        preds = commonLayer[1](preds)\n",
    "        commonLayer = preds\n",
    "\n",
    "    return commonLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS PLot utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ts_plot_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ts_plot_utils.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "import ts_utils\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plot(y, yh, x=None, title=\"\", scaler=None):\n",
    "    if (scaler):\n",
    "        y1  = scaler.inverse_transform(y) \n",
    "        yh1 = scaler.inverse_transform(yh) \n",
    "    else:\n",
    "        y1, yh1 = y, yh\n",
    "\n",
    "    x = x or range(max(len(y1),len(yh)))\n",
    "    \n",
    "    plt.scatter(x, y1,  marker='.', s=64, edgecolor='k', label=\"Y\")\n",
    "    plt.scatter(x, yh1, marker='x', s=64, edgecolor='k', label=\"$\\hat{y}$\")\n",
    "    plt.title(title)\n",
    "    plt.grid(1)\n",
    "    plt.legend()\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plotFeatureImportance(weights, labels):\n",
    "    plt.bar( x = range(len(weights)), height=weights)\n",
    "    if (labels):\n",
    "        print(labels)\n",
    "        axis = plt.gca()\n",
    "        axis.set_xticks(range(len(labels)))\n",
    "        axis.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "        _ = axis.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def eval_performance(model, trn_dataset, tst_dataset=None, metric_name=\"loss\"):\n",
    "    en = model.evaluate(trn_dataset)\n",
    "    if (tst_dataset):\n",
    "        et = model.evaluate(tst_dataset);\n",
    "    else:\n",
    "        et = [0] * len(en)\n",
    "\n",
    "    mi = max(0, model.metrics_names.index(metric_name))\n",
    "\n",
    "    return np.array(en).flat[mi], np.array(et).flat[mi]\n",
    "\n",
    "performance = {}\n",
    "def plot_performance(models, trn_dataset, tst_dataset=None, metric_name=\"loss\", performance = {}, reeval=0):\n",
    "    for m in models:\n",
    "        if (not reeval and performance.get(m.name, None)):\n",
    "            print(f\"Performance for {m.name} exists\")\n",
    "            continue;  # Dont evaluate if performance is already computed\n",
    "\n",
    "        performance[m.name] = eval_performance(m, trn_dataset, tst_dataset, metric_name)\n",
    "\n",
    "    if (len(performance) <= 0 ):\n",
    "        print(\"No models to plot?\")\n",
    "\n",
    "    x = np.arange(len(performance))\n",
    "    width = 0.3\n",
    "    val_mae =  [v[0] for v in performance.values()]\n",
    "    test_mae = [v[1] for v in performance.values()]\n",
    "\n",
    "    plt.title( f\"Comparisons of '{metric_name}' : \")\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.bar(x - 0.17, val_mae, width,  label= f'Training {metric_name}')\n",
    "    plt.bar(x + 0.17, test_mae, width, label= f'Test {metric_name}')\n",
    "    plt.xticks(ticks=x, labels=performance.keys(), rotation=45)\n",
    "    _ = plt.legend()\n",
    "    \n",
    "    return performance\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plot_predictions(ydf, yhatdf, start=0, end=1024*1024, title=\"\"):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    for c in ydf.columns:\n",
    "        y1, p1 = ydf[c][start:end], yhatdf[c][start:end]\n",
    "        #plt.scatter( y1.index, y1, edgecolors='k', marker='o', label= f'{c}: y',    c='#2ca02c' )\n",
    "        #plt.scatter( p1.index, p1, edgecolors='k', marker='X', label= f'{c}: yhat', c='#ff7f0e')\n",
    "\n",
    "        plt.plot( y1.index, y1, '-.', label= f'{c}: y',    c='#2ca02c' )\n",
    "        plt.plot( p1.index, p1, '-.', label= f'{c}: yhat', c='#ff0000')\n",
    "\n",
    "        plt.title( title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def predict_and_plot( model, window_trn, window_tst, howmany=1024* 1024,\n",
    "                        plot_start=0, plot_end=1024*1024, df=None, scaler=None, label_slice=None):\n",
    "    y, yhat = None, None\n",
    "    if (window_tst is not None):\n",
    "        y, yhat = ts_utils.model_predict( model , window_tst,  y, yhat, howmany)\n",
    "    else:\n",
    "        y, yhat = ts_utils.model_predict( model , window_trn,  y, yhat, howmany)\n",
    "\n",
    "    if ( df is not None):\n",
    "        ydf = ts_utils.inverse_transform(y, scaler, label_slice, df)\n",
    "        pdf = ts_utils.inverse_transform(yhat, scaler, label_slice, df)\n",
    "    else:\n",
    "        ydf = pd.DataFrame(y   )\n",
    "        pdf = pd.DataFrame(yhat)\n",
    "\n",
    "    plot_predictions(ydf,pdf, plot_start, plot_end, title=f\"{model.name}\")\n",
    "\n",
    "    return ydf, pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Transformer \n",
    "\n",
    "This is custom column transformer to help in dataframe transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ts_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ts_transform.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "    Utility that is derived from Column transformer to do inverse_transform\n",
    "\n",
    "USAGE:\n",
    "    Easiest way to use it:\n",
    "\n",
    "        df = pd.read_csv(\"../data/processminer-rare-event-mts.csv.zip\", sep=\";\")\n",
    "        scaler, df2 = myColumnTransformer.scale_df(df1)\n",
    "\n",
    "    Use it scale other data: \n",
    "\n",
    "        scaler.fit(df1)\n",
    "\n",
    "'''\n",
    "class myColumnTransformer(ColumnTransformer):\n",
    "    def out_feature_names(self):\n",
    "\n",
    "        if( not self.verbose_feature_names_out ):\n",
    "            return self.get_feature_names_out()\n",
    "        else:\n",
    "            # Note Below is only valid if scaler is called without 'verbose_feature_names_out'\n",
    "            out_feats = []\n",
    "            for s,v in self.named_transformers_.items():\n",
    "                if ( type(v) == str):       # Problem arises when dont drop remainders\n",
    "                    out_feats.extend([v])\n",
    "                    continue\n",
    "\n",
    "                out_feats.extend(v.get_feature_names_out())\n",
    "\n",
    "            return  out_feats\n",
    "\n",
    "    def transform(self, df, returnDF=True):\n",
    "        ret = super().transform(df)\n",
    "        rdf = None\n",
    "        if (returnDF):\n",
    "            ret = pd.DataFrame(ret, columns= self.out_feature_names())\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def inverse_transform(self, sdf, inplace=False):\n",
    "        ret = sdf if (inplace) else pd.DataFrame()\n",
    "\n",
    "        for s,v in self.named_transformers_.items():\n",
    "            if (not hasattr(v, \"inverse_transform\")):\n",
    "                continue;\n",
    "            \n",
    "            fo = v.get_feature_names_out()\n",
    "            #print(f\"Inverting {fo} => {v.feature_names_in_} {set(fo).issubset(sdf.columns)} \")\n",
    "            if (not set(fo).issubset(sdf.columns)):\n",
    "                continue;\n",
    "            ret[v.feature_names_in_] = v.inverse_transform(sdf[fo])\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def save(self, file=\"my\", ext=\".scaler.pkl\"):\n",
    "        pickle.dump(self, open(f'{file}.scaler.pkl', 'wb'))\n",
    "\n",
    "    def load(self, file=\"my\", ext=\".scaler.pkl\"):\n",
    "        ret = pickle.load( open(f'{file}.scaler.pkl', 'rb'))\n",
    "        return ret\n",
    "\n",
    "\n",
    "    '''\n",
    "        Finds numeric and categorical columns from DF.\n",
    "        It returns empty lists if there are no columns matching the criteria.\n",
    "        For ex: if there are no categorical variables, it returns empty list in categori.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def find_cat_numerics_names(df, num_unique=5):\n",
    "        unique = df.nunique()\n",
    "        numerics = unique[unique  >  num_unique].index.to_list()\n",
    "        numerics = [ i for i in numerics if is_numeric_dtype(df[i]) ]\n",
    "        categori = unique[unique <= num_unique].index.to_list()\n",
    "\n",
    "        return numerics, categori\n",
    "\n",
    "    '''\n",
    "        Scale data frame\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def scale_df(df, num_unique=5, numericScaler= StandardScaler, numerics = None, categorical = None):\n",
    "        # Detect numerics if both not given; if one is given - we assume other is not required\n",
    "        if ( numerics is None and categorical is None):\n",
    "            numerics, categorical = myColumnTransformer.find_cat_numerics_names(df, num_unique)\n",
    "\n",
    "        scaler = myColumnTransformer( transformers= \n",
    "                [(n  ,  numericScaler(), [n] ) for n in numerics] +\n",
    "                [(\"categorical\",  OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), categorical)]\n",
    "            , remainder='drop',\n",
    "            verbose_feature_names_out = False)\n",
    "\n",
    "        x = scaler.fit_transform(df)\n",
    "        scaler.numerics = numerics\n",
    "        scaler.categorical = categorical\n",
    "\n",
    "        return scaler, pd.DataFrame(x, columns= scaler.out_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    Here is a test and how to use it.\n",
    "'''\n",
    "def testMyColumnTransformer():\n",
    "    df = pd.read_csv(\"../data/processminer-rare-event-mts.csv.zip\", sep=\";\")\n",
    "    nums, cats = myColumnTransformer.find_cat_numerics_names(df, 5)\n",
    "\n",
    "    # Lets take and example of first 5 columsn and cats columns for our test\n",
    "    # Eliminate first column because it is a time column\n",
    "    #\n",
    "    df1 = df[nums[1:5]+cats]\n",
    "    display(df1[0:4])\n",
    "    '''\n",
    "        time       x1\t       x2         x3\t       x4          x5       y  x61\n",
    "    --------------- -------     ---------   --------    --------    ---------   -  ---\n",
    "    0\t5/1/99 0:00\t0.376665\t-4.596435\t-4.095756\t13.497687\t-0.118830\t0\t0\n",
    "    1\t5/1/99 0:02\t0.475720\t-4.542502\t-4.018359\t16.230659\t-0.128733\t0\t0\n",
    "    2\t5/1/99 0:04\t0.363848\t-4.681394\t-4.353147\t14.127997\t-0.138636\t0\t0\n",
    "    3\t5/1/99 0:06\t0.301590\t-4.758934\t-4.023612\t13.161566\t-0.148142\t0\t0\n",
    "    4\t5/1/99 0:08\t0.265578\t-4.749928\t-4.333150\t15.267340\t-0.155314\t0\t0\n",
    "    '''\n",
    "\n",
    "    scaler, df2 = myColumnTransformer.scale_df(df1)\n",
    "    print(\"\\nScaled Dataframe:\")\n",
    "    display(df2[0:4])\n",
    "\n",
    "\n",
    "    # Now you can use scaler to scale other data - \n",
    "    # it will do the job as long as there are columns in dataframe matching i/p columns\n",
    "    # It correctly returns the columns in correct order as original order \n",
    "    #\n",
    "    # You can call transform set returnDF to False to get raw numpy\n",
    "    #\n",
    "    print(\"\\nOut of order columns still work correctly:\")\n",
    "    df3 = scaler.transform(df[df.columns[::-1]], True)\n",
    "    display(df3[0:4])\n",
    "\n",
    "\n",
    "    # It throwns an error if the expected column is not in the dataframe\n",
    "    # \n",
    "    cols = \"x1 x2 x3 x4 x7 y x61\"\n",
    "    try:\n",
    "        df3 = pd.DataFrame(scaler.transform(df[cols]), columns= scaler.out_feature_names())\n",
    "    except Exception as e:\n",
    "        print (f\"+++ EXCEPTION IS CORRECT KeyError  missing 'x5' {e} {type(e)}\")\n",
    "\n",
    "    print(\"\\nInverse dataframe correctly inverts it:\")\n",
    "    idf = scaler.inverse_transform(df3)\n",
    "    display(idf[0:4])\n",
    "\n",
    "    print('''\\nInverse dataframe correctly inverts it: \n",
    "    Does it do it if columns or out of order?\n",
    "    => It DOES!!! See below\n",
    "    ''')\n",
    "    odf = df3[df3.columns[::-1]]\n",
    "    display(odf[0:4])\n",
    "    idf = scaler.inverse_transform(odf)\n",
    "    display(idf[0:4])\n",
    "\n",
    "\n",
    "#testMyColumnTransformer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile \"ts_transformer.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pickle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Test the utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test window function\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(df_scaled_trn[df_scaled_trn.columns[:4]])\n",
    "wd = window(ds, 3, 2, slice(1,3), 1,1)\n",
    "for w in wd.take(3):\n",
    "    print(f\"{w[0].numpy()}\\n=>:\\n{w[1].numpy()} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Advanced Windowing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../data/jena_climate_2009_2016.csv.zip'\n",
    "file = '../data/processminer-rare-event-mts.csv.zip'\n",
    "df = pd.read_csv(file, sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, dfo = scale_df( df[df.columns[2:]] )\n",
    "dfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfo = dfo.iloc[:,[0,1,2,-1,-2]]\n",
    "ret = scaler.inverse_transform(sdfo)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.save(\"/tmp/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = scaler.load(\"/tmp/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfo = dfo.iloc[:,[0,1,2,-1,-2]]\n",
    "ret = scaler.inverse_transform(sdfo)\n",
    "ret"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
