{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd112195",
   "metadata": {
    "id": "dd112195"
   },
   "source": [
    "# Coding Exercise Part 1: NN from Scratch in Numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e1f3f",
   "metadata": {
    "id": "2d3e1f3f"
   },
   "source": [
    "### General Objective\n",
    "* Code a \"vanilla\" feedforward neural network from the scratch using Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4971c3",
   "metadata": {
    "id": "bd4971c3"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seed: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f8472",
   "metadata": {
    "id": "de9f8472"
   },
   "source": [
    "### 1. Implement Non-linear Activation Functions\n",
    "In this exercise, you will implement some of the most commonly used non-linear activation functions in neural networks. These functions introduce non-linearity into the model, allowing it to learn from the error and make adjustments, which is essential for learning complex patterns.\n",
    "\n",
    "**Objectives:**\n",
    "* Implement the Rectified Linear Unit (ReLU) activation function.\n",
    "* Implement the derivative of the ReLU function.\n",
    "* Implement the Sigmoid activation function.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "**ReLU (Rectified Linear Unit):** The function itself is max(0, x), meaning that if the input is positive, it returns the input, and if it's negative or zero, it returns zero. It's one of the most widely used activation functions in deep learning models. More details can be found [here](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)).\n",
    "\n",
    "**Sigmoid:** It's an S-shaped curve that can take any real-valued number and map it between 0 and 1. It's especially useful for models where we have to predict the probability as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba80913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the ReLU of x.\n",
    "\n",
    "    Args:\n",
    "    - x: A numpy array of any shape.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the same shape as x, where each element is the ReLU of the corresponding element of x.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the derivative of ReLU of x.\n",
    "\n",
    "    Args:\n",
    "    - x: A numpy array of any shape.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the same shape as x, where each element is the derivative of ReLU of the corresponding element of x.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x.\n",
    "\n",
    "    Args:\n",
    "    - x: A numpy array of any shape.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the same shape as x, where each element is the sigmoid of the corresponding element of x.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425c15c",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* Ensure that the functions can handle numpy arrays of any shape.\n",
    "* Avoid using loops; instead, utilize numpy's vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d671aa",
   "metadata": {
    "id": "b3d671aa"
   },
   "source": [
    "### 2. Implement Loss Function (Binary Cross Entropy Loss)\n",
    "\n",
    "In this exercise, you will implement the Binary Cross Entropy (BCE) loss function, which is commonly used in binary classification tasks. The BCE loss measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "**Objectives:** \n",
    "* Implement the Binary Cross Entropy loss function.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "**Binary Cross Entropy Loss:** It's a loss function used for binary classification problems. The BCE loss increases as the predicted probability diverges from the actual label. The mathematical formula for BCE is given by:\n",
    "\n",
    "$L_{B C E}=-\\frac{1}{n} \\sum_{i=1}^n\\left(Y_i \\cdot \\log \\hat{Y}_i+\\left(1-Y_i\\right) \\cdot \\log \\left(1-\\hat{Y}_i\\right)\\right)$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $n$ is the number of samples.\n",
    "* $Y_i$ is the actual label (0 or 1).\n",
    "* $\\hat{Y}_i$ is the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b8417",
   "metadata": {
    "id": "b04b8417"
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the Binary Cross Entropy loss.\n",
    "\n",
    "    Args:\n",
    "    - y: A numpy array of shape (n,) containing the true labels (0 or 1).\n",
    "    - y_hat: A numpy array of shape (n,) containing the predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar representing the BCE loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    y_hat   = np.clip(y_hat, epsilon, 1 - epsilon) # clipping (>0) to avoid NaNs in log\n",
    "    \n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1c1bf",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* Ensure that the function can handle numpy arrays of any shape.\n",
    "* To avoid `NaNs` in the logarithm operation when $\\hat{Y}_i$ is exactly 0, clip the values of $\\hat{Y}_i$ to a small value above 0 using [`np.clip`](https://numpy.org/doc/stable/reference/generated/numpy.clip.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607401eb",
   "metadata": {
    "id": "607401eb"
   },
   "source": [
    "### 3. Implement Accuracy Score\n",
    "\n",
    "In this exercise, you will implement the accuracy score, which is a common metric used to evaluate the performance of classification models.\n",
    "\n",
    "**Objectives:**\n",
    "Implement the accuracy score function.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "**Accuracy Score:** It's a metric that calculates the fraction of predictions our model got right. It is defined as the number of correct predictions divided by the total number of predictions. The mathematical formula for accuracy is given by:\n",
    "\n",
    "$\\text { Accuracy }=\\frac{\\text { Number of Correct Predictions }}{\\text { Total Number of Predictions }}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d38ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the accuracy score.\n",
    "\n",
    "    Args:\n",
    "    - y: A numpy array of shape (n,) containing the true labels.\n",
    "    - y_hat: A numpy array of shape (n,) containing the predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar representing the accuracy score.\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d43b5",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* Ensure that the function can handle numpy arrays of any shape.\n",
    "* The function should return the fraction of correct predictions.\n",
    "* Avoid using loops; instead, utilize numpy's vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9bab7",
   "metadata": {
    "id": "b6f9bab7"
   },
   "source": [
    "### 4. Implement Neural Network class\n",
    "\n",
    "In this exercise, you will implement a simple feed-forward neural network with one hidden layer. The neural network will be used for binary classification tasks.\n",
    "\n",
    "**Objectives:**\n",
    "* Implement the initialization method for the neural network.\n",
    "* Implement the forward propagation step.\n",
    "\n",
    "**Background:**\n",
    "**Feed-forward Neural Network:** This is a type of artificial neural network where the connections between the nodes do not form a cycle. In this exercise, the neural network will have an input layer, one hidden layer, and an output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78000fd",
   "metadata": {
    "id": "b78000fd"
   },
   "outputs": [],
   "source": [
    "# Neural Network class\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the given sizes.\n",
    "\n",
    "        Args:\n",
    "        - input_size: Integer, the number of input features.\n",
    "        - hidden_size: Integer, the number of neurons in the hidden layer.\n",
    "        - output_size: Integer, the number of neurons in the output layer (usually 1 for binary classification).\n",
    "\n",
    "        Attributes to initialize:\n",
    "        - W1, b1: Weights and bias for the input to hidden layer transformation.\n",
    "        - W2, b2: Weights and bias for the hidden to output layer transformation.\n",
    "        - input_size, hidden_size, output_size: Store the sizes.\n",
    "        - loss_tracker: A list to keep track of the loss during training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parameters dictionary holding weights and bias\n",
    "        self.W1 = []\n",
    "        self.b1 = []\n",
    "        self.W2 = []\n",
    "        self.b2 = []\n",
    "\n",
    "        # Store NN shape (3 layers: input, hidden and output)\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Loss x iteration tracking\n",
    "        self.loss_tracker = []\n",
    "        \n",
    "        # Your code here (Initialize weights)\n",
    "\n",
    "    # Weights initialization using random normal distribution\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases for the neural network using a random normal distribution for weights \n",
    "        and zeros for biases.\n",
    "\n",
    "        Attributes to initialize:\n",
    "        - W1, b1: Weights and bias for the input to hidden layer transformation.\n",
    "        - W2, b2: Weights and bias for the hidden to output layer transformation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        # Initialize first layer\n",
    "        self.W1 = # ... \n",
    "        self.b1 = # ... \n",
    "\n",
    "        # Initialize second layer\n",
    "        self.W2 = # ... \n",
    "        self.b2 = # ... \n",
    "        \n",
    "    # Trains the neural network using the specified data (X) and labels (y)\n",
    "    def train(self, X, y, learning_rate=0.001, epochs=1000):\n",
    "         # Reset weights and bias\n",
    "        self.initialize_weights()\n",
    "\n",
    "        # Train a number of iterations\n",
    "        for epoch in range(epochs):\n",
    "            # Compute forward propagation\n",
    "            Z1, A1, Z2, A2 = self.forward_propagation(X)\n",
    "\n",
    "            # Compute loss value\n",
    "            loss = binary_cross_entropy_loss(y, A2)\n",
    "            self.loss_tracker.append(loss)\n",
    "\n",
    "            # Perform backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, y, Z1, A1, A2)\n",
    "\n",
    "            # Update the model parameters\n",
    "            self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "            # Print status\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} - Loss: {loss}\")\n",
    "\n",
    "    # Perform forward propagation step\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward propagation step.\n",
    "\n",
    "        Args:\n",
    "        - X: A numpy array of shape (input_size, n) where n is the number of samples.\n",
    "\n",
    "        Returns:\n",
    "        - Z1, A1, Z2, A2: Intermediate values from the forward propagation.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "\n",
    "        Z1 = # ... \n",
    "        A1 = # ... \n",
    "        Z2 = # ... \n",
    "        A2 = # ... \n",
    "        \n",
    "        return Z1, A1, Z2, A2\n",
    "    \n",
    "    # Perform backward propagation step\n",
    "    def backward_propagation(self, X, y, Z1, A1, A2):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to Z2 (output of the second layer)\n",
    "        dZ2 = A2 - y\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to W2 (weights of the second layer)\n",
    "        # Values are scaled by m (dataset size). This is done to ensure that the update\n",
    "        # to the weights and biases (during the gradient descent step) is based on the\n",
    "        # average error across all examples, rather than the sum of the errors.\n",
    "        dW2 = A1 @ dZ2.T / m\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to b2 (bias of the second layer)\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m # Scaled based on the size of the dataset\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to Z1 (output of the first layer before activation)\n",
    "        # This involves the chain rule, taking into account the derivative of the activation function\n",
    "        dZ1 = self.W2 @ dZ2 * relu_derivative(Z1)\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to W1 (weights of the first layer)\n",
    "        dW1 = X @ dZ1.T / m # Scaled based on the size of the dataset\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to b1 (bias of the first layer)\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m # Scaled based on the size of the dataset\n",
    "\n",
    "        # Return the gradients for the weights and biases of both layers\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    # Update parameters (in the direction opposite to the derivative)\n",
    "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # Predicts on test data\n",
    "        _, _, _, A2 = self.forward_propagation(X)\n",
    "        return (A2 > 0.5).astype(int)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        # Plots the loss curve\n",
    "        plt.plot(self.loss_tracker)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss (BCE)\")\n",
    "        plt.title(\"Loss curve\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67cb90",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "* Ensure that the methods can handle numpy arrays of any shape.\n",
    "* The weights should be initialized using a random normal distribution and biases should be initialized to zeros.\n",
    "* Avoid using loops; instead, utilize numpy's vectorized operations.\n",
    "* The provided code contains other methods like train, backward_propagation, update_parameters, predict, and plot_loss. You don't need to modify these methods for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a0f73",
   "metadata": {
    "id": "022a0f73"
   },
   "source": [
    "### 5. Load dataset\n",
    "We will work with the Haberman’s Survival Dataset. The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer. There are 306 items (patients). There are three predictor variables (age, year of operation, number of detected nodes). The variable to predict is encoded as 1 (survived) or 2 (died). See [\n",
    "Haberman's Survival Dataset](https://archive.ics.uci.edu/dataset/43/haberman+s+survival)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d62af",
   "metadata": {
    "id": "ad6d62af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/emmanueliarussi/DU-DeepLearning/main/week_2/haberman_data/haberman.data'\n",
    "headers =  ['age', 'year','nodes','y']\n",
    "haberman_df  = pd.read_csv(url, sep=',', names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde97bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ddde97bc",
    "outputId": "d07dae01-509c-4454-e1ae-cbaeec99670b"
   },
   "outputs": [],
   "source": [
    "haberman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6e120",
   "metadata": {
    "id": "b8a6e120"
   },
   "outputs": [],
   "source": [
    "# Convert pandas dataframe into numpy arrays\n",
    "x       = haberman_df.drop(columns=['y']).values[1:]\n",
    "y_label = haberman_df['y'].values[1:].reshape(x.shape[0], 1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ddc83",
   "metadata": {
    "id": "2b4ddc83"
   },
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y_label, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c5f67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e9c5f67",
    "outputId": "33ecf5e2-1be1-4d4b-aeac-0c87c7ff1f11"
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(xtrain)\n",
    "xtrain = sc.transform(xtrain)\n",
    "xtest  = sc.transform(xtest)\n",
    "\n",
    "print(\"Shape of train set is {}\".format(xtrain.shape))\n",
    "print(\"Shape of test set is {}\".format(xtest.shape))\n",
    "print(\"Shape of train label is {}\".format(ytrain.shape))\n",
    "print(\"Shape of test labels is {}\".format(ytest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c8c4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "409c8c4b",
    "outputId": "9b8645ad-275f-4095-e19a-cb7e9c6c9378"
   },
   "outputs": [],
   "source": [
    "# Create and train the Neural Network model\n",
    "nn = NeuralNetwork(input_size=3, hidden_size=5, output_size=1)\n",
    "nn.train(xtrain.T, ytrain.T, epochs=5000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bd120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "6a1bd120",
    "outputId": "d5dfa505-6b33-4e27-c120-6b8ea8735bbc"
   },
   "outputs": [],
   "source": [
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c05ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0c05ee4",
    "outputId": "24d1fb0e-7fe4-4430-9a9b-83343b94b5f0"
   },
   "outputs": [],
   "source": [
    "# Compare the training vs. testing data performance\n",
    "train_pred = nn.predict(xtrain.T)\n",
    "test_pred  = nn.predict(xtest.T)\n",
    "\n",
    "print(\"Train accuracy is {0:.2f}\".format(accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {0:.2f}\".format(accuracy(ytest, test_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
