{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd112195",
   "metadata": {
    "id": "dd112195"
   },
   "source": [
    "# Coding Exercise Part 1: NN from Scratch in Numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e1f3f",
   "metadata": {
    "id": "2d3e1f3f"
   },
   "source": [
    "### General Objective\n",
    "* Code a \"vanilla\" feedforward neural network from the scratch using Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4971c3",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695304753679,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "bd4971c3"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seed: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f8472",
   "metadata": {
    "id": "de9f8472"
   },
   "source": [
    "### 1. Implement Non-linear Activation Functions\n",
    "In this exercise, you will implement some of the most commonly used non-linear activation functions in neural networks. These functions introduce non-linearity into the model, allowing it to learn from the error and make adjustments, which is essential for learning complex patterns.\n",
    "\n",
    "**Objectives:**\n",
    "* Implement the Rectified Linear Unit (ReLU) activation function.\n",
    "* Implement the derivative of the ReLU function.\n",
    "* Implement the Sigmoid activation function.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "**ReLU (Rectified Linear Unit):** The function itself is max(0, x), meaning that if the input is positive, it returns the input, and if it's negative or zero, it returns zero. It's one of the most widely used activation functions in deep learning models. More details can be found [here](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)).\n",
    "\n",
    "**Sigmoid:** It's an S-shaped curve that can take any real-valued number and map it between 0 and 1. It's especially useful for models where we have to predict the probability as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191d9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the ReLU of x.\n",
    "\n",
    "    Args:\n",
    "    - x: A numpy array of any shape.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the same shape as x, where each element is the ReLU of the corresponding element of x.\n",
    "    \"\"\"\n",
    "    # Relu activation function\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2f9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the derivative of ReLU of x.\n",
    "\n",
    "    Args:\n",
    "    - x: A numpy array of any shape.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the same shape as x, where each element is the derivative of ReLU of the corresponding element of x.\n",
    "    \"\"\"\n",
    "    # Relu derivative function\n",
    "    x[x<=0] = 0\n",
    "    x[x>0]  = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08231001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x.\n",
    "\n",
    "    Args:\n",
    "    - x: A numpy array of any shape.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the same shape as x, where each element is the sigmoid of the corresponding element of x.\n",
    "    \"\"\"\n",
    "    # Sigmoid activation function\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7cbdb",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* Ensure that the functions can handle numpy arrays of any shape.\n",
    "* Avoid using loops; instead, utilize numpy's vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d671aa",
   "metadata": {
    "id": "b3d671aa"
   },
   "source": [
    "### 2. Implement Loss Function (Binary Cross Entropy Loss)\n",
    "\n",
    "In this exercise, you will implement the Binary Cross Entropy (BCE) loss function, which is commonly used in binary classification tasks. The BCE loss measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "**Objectives:** \n",
    "* Implement the Binary Cross Entropy loss function.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "**Binary Cross Entropy Loss:** It's a loss function used for binary classification problems. The BCE loss increases as the predicted probability diverges from the actual label. The mathematical formula for BCE is given by:\n",
    "\n",
    "$L_{B C E}=-\\frac{1}{n} \\sum_{i=1}^n\\left(Y_i \\cdot \\log \\hat{Y}_i+\\left(1-Y_i\\right) \\cdot \\log \\left(1-\\hat{Y}_i\\right)\\right)$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $n$ is the number of samples.\n",
    "* $Y_i$ is the actual label (0 or 1).\n",
    "* $\\hat{Y}_i$ is the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e11355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the Binary Cross Entropy loss.\n",
    "\n",
    "    Args:\n",
    "    - y: A numpy array of shape (n,) containing the true labels (0 or 1).\n",
    "    - y_hat: A numpy array of shape (n,) containing the predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar representing the BCE loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    y_hat   = np.clip(y_hat, epsilon, 1 - epsilon) # clipping (>0) to avoid NaNs in log\n",
    "    \n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e29006",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* Ensure that the function can handle numpy arrays of any shape.\n",
    "* To avoid `NaNs` in the logarithm operation when $\\hat{Y}_i$ is exactly 0, clip the values of $\\hat{Y}_i$ to a small value above 0 using [`np.clip`](https://numpy.org/doc/stable/reference/generated/numpy.clip.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607401eb",
   "metadata": {
    "id": "607401eb"
   },
   "source": [
    "### 3. Implement Accuracy Score\n",
    "\n",
    "In this exercise, you will implement the accuracy score, which is a common metric used to evaluate the performance of classification models.\n",
    "\n",
    "**Objectives:**\n",
    "Implement the accuracy score function.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "**Accuracy Score:** It's a metric that calculates the fraction of predictions our model got right. It is defined as the number of correct predictions divided by the total number of predictions. The mathematical formula for accuracy is given by:\n",
    "\n",
    "$\\text { Accuracy }=\\frac{\\text { Number of Correct Predictions }}{\\text { Total Number of Predictions }}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76994361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the accuracy score.\n",
    "\n",
    "    Args:\n",
    "    - y: A numpy array of shape (n,) containing the true labels.\n",
    "    - y_hat: A numpy array of shape (n,) containing the predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar representing the accuracy score.\n",
    "    \"\"\"\n",
    "    # Computes the accuracy between the predicted labels and the truth labels\n",
    "    acc = sum(y == y_hat) / len(y)  # fraction of predictions our model got right\n",
    "    return acc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc61ed4",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* Ensure that the function can handle numpy arrays of any shape.\n",
    "* The function should return the fraction of correct predictions.\n",
    "* Avoid using loops; instead, utilize numpy's vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9bab7",
   "metadata": {
    "id": "b6f9bab7"
   },
   "source": [
    "### 4. Implement Neural Network class\n",
    "\n",
    "In this exercise, you will implement a simple feed-forward neural network with one hidden layer. The neural network will be used for binary classification tasks.\n",
    "\n",
    "**Objectives:**\n",
    "* Implement the initialization method for the neural network.\n",
    "* Implement the forward propagation step.\n",
    "\n",
    "**Background:**\n",
    "**Feed-forward Neural Network:** This is a type of artificial neural network where the connections between the nodes do not form a cycle. In this exercise, the neural network will have an input layer, one hidden layer, and an output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78000fd",
   "metadata": {
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1695305012668,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "b78000fd"
   },
   "outputs": [],
   "source": [
    "# Neural Network class\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the given sizes.\n",
    "\n",
    "        Args:\n",
    "        - input_size: Integer, the number of input features.\n",
    "        - hidden_size: Integer, the number of neurons in the hidden layer.\n",
    "        - output_size: Integer, the number of neurons in the output layer (usually 1 for binary classification).\n",
    "\n",
    "        Attributes to initialize:\n",
    "        - W1, b1: Weights and bias for the input to hidden layer transformation.\n",
    "        - W2, b2: Weights and bias for the hidden to output layer transformation.\n",
    "        - input_size, hidden_size, output_size: Store the sizes.\n",
    "        - loss_tracker: A list to keep track of the loss during training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parameters dictionary holding weights and bias\n",
    "        self.W1 = []\n",
    "        self.b1 = []\n",
    "        self.W2 = []\n",
    "        self.b2 = []\n",
    "\n",
    "        # Store NN shape (3 layers: input, hidden and output)\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights\n",
    "        self.initialize_weights()\n",
    "\n",
    "        # Loss x iteration tracking\n",
    "        self.loss_tracker = []\n",
    "\n",
    "    # Weights initialization using random normal distribution\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases for the neural network using a random normal distribution for weights \n",
    "        and zeros for biases.\n",
    "\n",
    "        Attributes to initialize:\n",
    "        - W1, b1: Weights and bias for the input to hidden layer transformation.\n",
    "        - W2, b2: Weights and bias for the hidden to output layer transformation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize first layer\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((self.hidden_size,1))\n",
    "\n",
    "        # Initialize second layer\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "\n",
    "    # Trains the neural network using the specified data (X) and labels (y)\n",
    "    def train(self, X, y, learning_rate=0.001, epochs=1000):\n",
    "         # Reset weights and bias\n",
    "        self.initialize_weights()\n",
    "\n",
    "        # Train a number of iterations\n",
    "        for epoch in range(epochs):\n",
    "            # Compute forward propagation\n",
    "            Z1, A1, Z2, A2 = self.forward_propagation(X)\n",
    "\n",
    "            # Compute loss value\n",
    "            loss = binary_cross_entropy_loss(y, A2)\n",
    "            self.loss_tracker.append(loss)\n",
    "\n",
    "            # Perform backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward_propagation(X, y, Z1, A1, A2)\n",
    "\n",
    "            # Update the model parameters\n",
    "            self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "            # Print status\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} - Loss: {loss}\")\n",
    "\n",
    "    # Perform forward propagation step\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward propagation step.\n",
    "\n",
    "        Args:\n",
    "        - X: A numpy array of shape (input_size, n) where n is the number of samples.\n",
    "\n",
    "        Returns:\n",
    "        - Z1, A1, Z2, A2: Intermediate values from the forward propagation.\n",
    "        \"\"\"\n",
    "        Z1 = self.W1.T @ X + self.b1  # Linear activation\n",
    "        A1 = relu(Z1)               # Non-linear activation\n",
    "        Z2 = self.W2.T @ A1 + self.b2 # Linear activation\n",
    "        A2 = sigmoid(Z2)            # Non-linear activation\n",
    "\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    # Perform backward propagation step\n",
    "    def backward_propagation(self, X, y, Z1, A1, A2):\n",
    "        # Get the number of training examples\n",
    "        m = X.shape[1]\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to Z2 (output of the second layer)\n",
    "        dZ2 = A2 - y\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to W2 (weights of the second layer)\n",
    "        # Values are scaled by m (dataset size). This is done to ensure that the update\n",
    "        # to the weights and biases (during the gradient descent step) is based on the\n",
    "        # average error across all examples, rather than the sum of the errors.\n",
    "        dW2 = A1 @ dZ2.T / m\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to b2 (bias of the second layer)\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m # Scaled based on the size of the dataset\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to Z1 (output of the first layer before activation)\n",
    "        # This involves the chain rule, taking into account the derivative of the activation function\n",
    "        dZ1 = self.W2 @ dZ2 * relu_derivative(Z1)\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to W1 (weights of the first layer)\n",
    "        dW1 = X @ dZ1.T / m # Scaled based on the size of the dataset\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to b1 (bias of the first layer)\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m # Scaled based on the size of the dataset\n",
    "\n",
    "        # Return the gradients for the weights and biases of both layers\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    # Update parameters (in the direction opposite to the derivative)\n",
    "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        # Predicts on test data\n",
    "        _, _, _, A2 = self.forward_propagation(X)\n",
    "        return (A2 > 0.5).astype(int)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        # Plots the loss curve\n",
    "        plt.plot(self.loss_tracker)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss (BCE)\")\n",
    "        plt.title(\"Loss curve\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7658c",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "* Ensure that the methods can handle numpy arrays of any shape.\n",
    "* The weights should be initialized using a random normal distribution and biases should be initialized to zeros.\n",
    "* Avoid using loops; instead, utilize numpy's vectorized operations.\n",
    "* The provided code contains other methods like train, backward_propagation, update_parameters, predict, and plot_loss. You don't need to modify these methods for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a0f73",
   "metadata": {
    "id": "022a0f73"
   },
   "source": [
    "### 5. Load dataset\n",
    "We will work with the Haberman’s Survival Dataset. The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer. There are 306 items (patients). There are three predictor variables (age, year of operation, number of detected nodes). The variable to predict is encoded as 0 (survived) or 1 (died). See [\n",
    "Haberman's Survival Dataset](https://archive.ics.uci.edu/dataset/43/haberman+s+survival)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6d62af",
   "metadata": {
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1695305020409,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "ad6d62af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/emmanueliarussi/DU-DeepLearning/main/week_2/haberman_data/haberman.data'\n",
    "headers =  ['age', 'year','nodes','y']\n",
    "haberman_df  = pd.read_csv(url, sep=',', names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddde97bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1695305023190,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "ddde97bc",
    "outputId": "55da5f14-f497-4929-a947-5f1cf6484a3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>year</th>\n",
       "      <th>nodes</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>year</td>\n",
       "      <td>nodes</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>75</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>76</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>78</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>83</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  year  nodes  y\n",
       "0    age  year  nodes  y\n",
       "1     30    64      1  0\n",
       "2     30    62      3  0\n",
       "3     30    65      0  0\n",
       "4     31    59      2  0\n",
       "..   ...   ...    ... ..\n",
       "302   75    62      1  0\n",
       "303   76    67      0  0\n",
       "304   77    65      3  0\n",
       "305   78    65      1  1\n",
       "306   83    58      2  1\n",
       "\n",
       "[307 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haberman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a6e120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1695305134634,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "b8a6e120",
    "outputId": "ac9a0aae-5cad-4bd3-8d33-c5570984f3a9"
   },
   "outputs": [],
   "source": [
    "# Convert pandas dataframe into numpy arrays\n",
    "x       = haberman_df.drop(columns=['y']).values[1:]\n",
    "y_label = haberman_df['y'].values[1:].reshape(x.shape[0], 1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4ddc83",
   "metadata": {
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1695305031218,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "2b4ddc83"
   },
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y_label, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e9c5f67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1695305032870,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "7e9c5f67",
    "outputId": "539d0d50-5865-4b99-e551-097dc4499b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (244, 3)\n",
      "Shape of test set is (62, 3)\n",
      "Shape of train label is (244, 1)\n",
      "Shape of test labels is (62, 1)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(xtrain)\n",
    "xtrain = sc.transform(xtrain)\n",
    "xtest  = sc.transform(xtest)\n",
    "\n",
    "print(\"Shape of train set is {}\".format(xtrain.shape))\n",
    "print(\"Shape of test set is {}\".format(xtest.shape))\n",
    "print(\"Shape of train label is {}\".format(ytrain.shape))\n",
    "print(\"Shape of test labels is {}\".format(ytest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "409c8c4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1695305038354,
     "user": {
      "displayName": "William Li",
      "userId": "04724194045580891958"
     },
     "user_tz": 420
    },
    "id": "409c8c4b",
    "outputId": "c436af97-34f8-4721-e5e7-ae4f1bba35b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5000 - Loss: 0.789918640660066\n",
      "Epoch 100/5000 - Loss: 0.6449801903146681\n",
      "Epoch 200/5000 - Loss: 0.6006388920067977\n",
      "Epoch 300/5000 - Loss: 0.5808316581512042\n",
      "Epoch 400/5000 - Loss: 0.5668901008248501\n",
      "Epoch 500/5000 - Loss: 0.555918635532661\n",
      "Epoch 600/5000 - Loss: 0.5477523029299053\n",
      "Epoch 700/5000 - Loss: 0.5413024867107006\n",
      "Epoch 800/5000 - Loss: 0.5358937874046175\n",
      "Epoch 900/5000 - Loss: 0.531241707393715\n",
      "Epoch 1000/5000 - Loss: 0.5272716492144564\n",
      "Epoch 1100/5000 - Loss: 0.5240822097065853\n",
      "Epoch 1200/5000 - Loss: 0.5213609448024722\n",
      "Epoch 1300/5000 - Loss: 0.5189933890066117\n",
      "Epoch 1400/5000 - Loss: 0.5168429251465918\n",
      "Epoch 1500/5000 - Loss: 0.5149108310079495\n",
      "Epoch 1600/5000 - Loss: 0.5131266930777045\n",
      "Epoch 1700/5000 - Loss: 0.5112115276764942\n",
      "Epoch 1800/5000 - Loss: 0.509378602976146\n",
      "Epoch 1900/5000 - Loss: 0.5078392733672137\n",
      "Epoch 2000/5000 - Loss: 0.5065234257379364\n",
      "Epoch 2100/5000 - Loss: 0.505323116358584\n",
      "Epoch 2200/5000 - Loss: 0.5043165890439102\n",
      "Epoch 2300/5000 - Loss: 0.5034347248385858\n",
      "Epoch 2400/5000 - Loss: 0.5026837823206187\n",
      "Epoch 2500/5000 - Loss: 0.5020060026234268\n",
      "Epoch 2600/5000 - Loss: 0.5014067477284282\n",
      "Epoch 2700/5000 - Loss: 0.5009236950772301\n",
      "Epoch 2800/5000 - Loss: 0.5004968069447584\n",
      "Epoch 2900/5000 - Loss: 0.500098897684851\n",
      "Epoch 3000/5000 - Loss: 0.4996360155656392\n",
      "Epoch 3100/5000 - Loss: 0.4992003591011812\n",
      "Epoch 3200/5000 - Loss: 0.4987879304478838\n",
      "Epoch 3300/5000 - Loss: 0.49838183471450587\n",
      "Epoch 3400/5000 - Loss: 0.4979754692581987\n",
      "Epoch 3500/5000 - Loss: 0.4975778741457038\n",
      "Epoch 3600/5000 - Loss: 0.4972043185452957\n",
      "Epoch 3700/5000 - Loss: 0.49683343328023527\n",
      "Epoch 3800/5000 - Loss: 0.4964796969343984\n",
      "Epoch 3900/5000 - Loss: 0.49614377731683973\n",
      "Epoch 4000/5000 - Loss: 0.49582409241908654\n",
      "Epoch 4100/5000 - Loss: 0.4955470482434028\n",
      "Epoch 4200/5000 - Loss: 0.49528186298797805\n",
      "Epoch 4300/5000 - Loss: 0.49501798435018207\n",
      "Epoch 4400/5000 - Loss: 0.4947569396301863\n",
      "Epoch 4500/5000 - Loss: 0.4945301569036818\n",
      "Epoch 4600/5000 - Loss: 0.49430446837323627\n",
      "Epoch 4700/5000 - Loss: 0.4940614952813657\n",
      "Epoch 4800/5000 - Loss: 0.49382598085819274\n",
      "Epoch 4900/5000 - Loss: 0.49359757725493403\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Neural Network model\n",
    "nn = NeuralNetwork(input_size=3, hidden_size=5, output_size=1)\n",
    "nn.train(xtrain.T, ytrain.T, epochs=5000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1bd120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "6a1bd120",
    "outputId": "d5dfa505-6b33-4e27-c120-6b8ea8735bbc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c05ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0c05ee4",
    "outputId": "24d1fb0e-7fe4-4430-9a9b-83343b94b5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 0.75\n",
      "Test accuracy is 0.68\n"
     ]
    }
   ],
   "source": [
    "# Compare the training vs. testing data performance\n",
    "train_pred = nn.predict(xtrain.T)\n",
    "test_pred  = nn.predict(xtest.T)\n",
    "\n",
    "print(\"Train accuracy is {0:.2f}\".format(accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {0:.2f}\".format(accuracy(ytest, test_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
